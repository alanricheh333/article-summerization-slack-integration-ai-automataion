from groq import Groq  # type: ignore
from env import GROQ


class LLM:
    def __init__(self):
        self.client = Groq(api_key=GROQ)

    def create_summary_prompt(self, article):
        """
        Generates a summary for the article using the LLM.

        Args:
            article (dict): A dictionary containing 'title', 'link', and 'content' of the article.

        Returns:
            str: The generated summary or an error message.
        """
        try:
            # Call the LLM API for summarization
            completion = self.client.chat.completions.create(
                model="llama-3.1-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "You are an experienced software engineer and content creator on LinkedIn. "
                            "Your goal is to summarize articles in an engaging and informative way for a "
                            "tech-savvy audience. Your tone should be approachable and professional but not "
                            "stiff or overly formal."
                        ),
                    },
                    {
                        "role": "user",
                        "content": (
                            "Summarize the following article into a LinkedIn-style post. Make it concise, engaging, and informative. "
                            "Avoid stiff language and keep it approachable for a broad tech audience. Include a title for the post and "
                            "mention the source of the article at the end. Remember, the scraped content may include irrelevant sentences, "
                            "so focus on the main ideas.\n\n"
                            f"Title: {article['title']}\n"
                            f"Link: {article['link']}\n"
                            f"Content:\n{article['content']}\n"
                        ),
                    },
                ],
                temperature=1,
                max_tokens=3500,
                top_p=1,
                stream=True,
                stop=None,
            )

            # Collect the streaming response
            output = ""
            for chunk in completion:
                output += chunk.choices[0].delta.content or ""

            return output

        except Exception as e:
            print(f"Error generating summary: {e}")
            return "An error occurred while generating the summary. Please try again later."


# Example usage
def main(article):
    """
    Main function to generate a summary for a given article.

    Args:
        article (dict): A dictionary containing 'title', 'link', and 'content' of the article.

    Returns:
        str: The summary generated by the LLM.
    """
    llm = LLM()
    summary = llm.create_summary_prompt(article)
    print("\nGenerated Summary:\n")
    print(summary)
    return summary


if __name__ == "__main__":
    # Example article dictionary
    article = {
        "title": "State of Trust Report | Vanta",
        "link": "https://www.vanta.com/state-of-trust?utm_campaign=state-of-trust-2024&utm_source=tldr&utm_medium=newsletter",
        "content": (
            "Find all your security and compliance content here.\n"
            "Get bite-sized definitions of the terms you need to know.\n"
            "Watch webinars and videos on trending security topics.\n"
            "Deepen your security knowledge and learn new skills.\n"
            "To uncover the latest trends shaping security and compliance, we surveyed 2,500 business and IT leaders across the US, UK, and Australia. "
            "Find out why third-party risk and AI are making it harder to build and demonstrate trust.\n"
            "More than half (55%) of organizations say that security risks have never been higher. At the same time, just 11% of a company's IT budget is dedicated "
            "to securityâ€”but leaders say it should be 17% in an ideal world.\n"
        ),
    }

    # Generate and print the summary
    main(article)
